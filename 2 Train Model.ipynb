{"cells":[{"cell_type":"code","source":["import pandas as pd\n","import torch\n","torch.__version__"],"outputs":[{"output_type":"execute_result","data":{"text/plain":["'1.11.0+cu113'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":2}],"execution_count":2,"metadata":{"gather":{"logged":1650944603309},"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"xS1Vqjr3Q7li","executionInfo":{"status":"ok","timestamp":1650967458494,"user_tz":-330,"elapsed":3692,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}},"outputId":"cbcfb55c-349d-448c-8047-200704fc989d"}},{"cell_type":"code","source":["# pip install torch-scatter -f 'https://data.pyg.org/whl/torch-1.10.2+cu102.html'"],"outputs":[],"execution_count":4,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650941169180},"id":"feOkTbxQQ7lp","executionInfo":{"status":"ok","timestamp":1650967591341,"user_tz":-330,"elapsed":503,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}}}},{"cell_type":"code","source":["train_data = pd.read_csv('data/train.tsv', sep='\\t')\n","train_data.head()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"id":"y5l2oSysOzOr","executionInfo":{"status":"ok","timestamp":1650965876640,"user_tz":-330,"elapsed":613,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}},"outputId":"979c68bf-db4d-4f4e-af56-6f7fec6fc1d6"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["       id  annotator  position  \\\n","0  nt-639          0         0   \n","1  nt-639          0         1   \n","2  nt-639          1         0   \n","3  nt-639          1         1   \n","4  nt-639          1         2   \n","\n","                                            question             table_file  \\\n","0                        where are the players from?  table_csv/203_149.csv   \n","1   which player went to louisiana state university?  table_csv/203_149.csv   \n","2                               who are the players?  table_csv/203_149.csv   \n","3                which ones are in the top 26 picks?  table_csv/203_149.csv   \n","4  and of those, who is from louisiana state univ...  table_csv/203_149.csv   \n","\n","                                  answer_coordinates  \\\n","0  ['(0, 4)', '(1, 4)', '(2, 4)', '(3, 4)', '(4, ...   \n","1                                         ['(0, 1)']   \n","2  ['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...   \n","3  ['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...   \n","4                                         ['(0, 1)']   \n","\n","                                         answer_text  \n","0  ['Louisiana State University', 'Valley HS (Las...  \n","1                                   ['Ben McDonald']  \n","2  ['Ben McDonald', 'Tyler Houston', 'Roger Salke...  \n","3  ['Ben McDonald', 'Tyler Houston', 'Roger Salke...  \n","4                                   ['Ben McDonald']  "],"text/html":["\n","  <div id=\"df-bd28aa80-9564-4239-b308-be190cd2e4d6\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>id</th>\n","      <th>annotator</th>\n","      <th>position</th>\n","      <th>question</th>\n","      <th>table_file</th>\n","      <th>answer_coordinates</th>\n","      <th>answer_text</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>nt-639</td>\n","      <td>0</td>\n","      <td>0</td>\n","      <td>where are the players from?</td>\n","      <td>table_csv/203_149.csv</td>\n","      <td>['(0, 4)', '(1, 4)', '(2, 4)', '(3, 4)', '(4, ...</td>\n","      <td>['Louisiana State University', 'Valley HS (Las...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>nt-639</td>\n","      <td>0</td>\n","      <td>1</td>\n","      <td>which player went to louisiana state university?</td>\n","      <td>table_csv/203_149.csv</td>\n","      <td>['(0, 1)']</td>\n","      <td>['Ben McDonald']</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>nt-639</td>\n","      <td>1</td>\n","      <td>0</td>\n","      <td>who are the players?</td>\n","      <td>table_csv/203_149.csv</td>\n","      <td>['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...</td>\n","      <td>['Ben McDonald', 'Tyler Houston', 'Roger Salke...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>nt-639</td>\n","      <td>1</td>\n","      <td>1</td>\n","      <td>which ones are in the top 26 picks?</td>\n","      <td>table_csv/203_149.csv</td>\n","      <td>['(0, 1)', '(1, 1)', '(2, 1)', '(3, 1)', '(4, ...</td>\n","      <td>['Ben McDonald', 'Tyler Houston', 'Roger Salke...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>nt-639</td>\n","      <td>1</td>\n","      <td>2</td>\n","      <td>and of those, who is from louisiana state univ...</td>\n","      <td>table_csv/203_149.csv</td>\n","      <td>['(0, 1)']</td>\n","      <td>['Ben McDonald']</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bd28aa80-9564-4239-b308-be190cd2e4d6')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-bd28aa80-9564-4239-b308-be190cd2e4d6 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-bd28aa80-9564-4239-b308-be190cd2e4d6');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":14}]},{"cell_type":"markdown","source":["As you can see, each row corresponds to a question related to a table.\n","\n","The position column identifies whether the question is the first, second, ... in a sequence of questions related to a table.\n","\n","The table_file column identifies the name of the table file, which refers to a CSV file in the table_csv directory.\n","\n","The answer_coordinates and answer_text columns indicate the answer to the question. The answer_coordinates is a list of tuples, each tuple being a (row_index, column_index) pair. The answer_text column is a list of strings, indicating the cell values.\n","\n","However, the answer_coordinates and answer_text columns are currently not recognized as real Python lists of Python tuples and strings respectively. Let us convert it to real python list of tuples"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"EfGI30FSQ7ly"}},{"cell_type":"code","source":["import ast\n","\n","def _parse_answer_coordinates(answer_coordinate_str):\n","  \"\"\"\n","  Parses the answer_coordinates of a question.\n","  Args:\n","    answer_coordinate_str: A string representation of a Python list of tuple\n","      strings.\n","      For example: \"['(1, 4)','(1, 3)', ...]\"\n","  \"\"\"\n","\n","  try:\n","    answer_coordinates = []\n","    # make a list of strings\n","    coords = ast.literal_eval(answer_coordinate_str)\n","    # parse each string as a tuple\n","    for row_index, column_index in sorted(\n","        ast.literal_eval(coord) for coord in coords):\n","      answer_coordinates.append((row_index, column_index))\n","  except SyntaxError:\n","    raise ValueError('Unable to evaluate %s' % answer_coordinate_str)\n","  \n","  return answer_coordinates\n","\n","\n","def _parse_answer_text(answer_text):\n","  \"\"\"\n","  Populates the answer_texts field of `answer` by parsing `answer_text`.\n","  Args:\n","    answer_text: A string representation of a Python list of strings.\n","      For example: \"[u'test', u'hello', ...]\"\n","    answer: an Answer object.\n","  \"\"\"\n","  try:\n","    answer = []\n","    for value in ast.literal_eval(answer_text):\n","      answer.append(value)\n","  except SyntaxError:\n","    raise ValueError('Unable to evaluate %s' % answer_text)\n","\n","  return answer\n","\n","train_data['answer_coordinates'] = train_data['answer_coordinates'].apply(lambda coords_str: _parse_answer_coordinates(coords_str))\n","train_data['answer_text'] = train_data['answer_text'].apply(lambda txt: _parse_answer_text(txt))\n","\n","train_data.head(10)"],"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"         id  annotator  position  \\\n0    nt-639          0         0   \n1    nt-639          0         1   \n2    nt-639          1         0   \n3    nt-639          1         1   \n4    nt-639          1         2   \n5    nt-639          2         0   \n6    nt-639          2         1   \n7  nt-11649          0         0   \n8  nt-11649          0         1   \n9  nt-11649          0         2   \n\n                                            question             table_file  \\\n0                        where are the players from?  table_csv/203_149.csv   \n1   which player went to louisiana state university?  table_csv/203_149.csv   \n2                               who are the players?  table_csv/203_149.csv   \n3                which ones are in the top 26 picks?  table_csv/203_149.csv   \n4  and of those, who is from louisiana state univ...  table_csv/203_149.csv   \n5                 who are the players in the top 26?  table_csv/203_149.csv   \n6  of those, which one was from louisiana state u...  table_csv/203_149.csv   \n7               what are all the names of the teams?  table_csv/204_135.csv   \n8              of these, which teams had any losses?  table_csv/204_135.csv   \n9     of these teams, which had more than 21 losses?  table_csv/204_135.csv   \n\n                                  answer_coordinates  \\\n0  [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...   \n1                                           [(0, 1)]   \n2  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n3  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n4                                           [(0, 1)]   \n5  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n6                                           [(0, 1)]   \n7  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n8  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n9                                          [(15, 1)]   \n\n                                         answer_text  \n0  [Louisiana State University, Valley HS (Las Ve...  \n1                                     [Ben McDonald]  \n2  [Ben McDonald, Tyler Houston, Roger Salkeld, J...  \n3  [Ben McDonald, Tyler Houston, Roger Salkeld, J...  \n4                                     [Ben McDonald]  \n5  [Ben McDonald, Tyler Houston, Roger Salkeld, J...  \n6                                     [Ben McDonald]  \n7  [Cordoba CF, CD Malaga, Granada CF, UD Las Pal...  \n8  [Cordoba CF, CD Malaga, Granada CF, UD Las Pal...  \n9                                 [CD Villarrobledo]  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>annotator</th>\n      <th>position</th>\n      <th>question</th>\n      <th>table_file</th>\n      <th>answer_coordinates</th>\n      <th>answer_text</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nt-639</td>\n      <td>0</td>\n      <td>0</td>\n      <td>where are the players from?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...</td>\n      <td>[Louisiana State University, Valley HS (Las Ve...</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nt-639</td>\n      <td>0</td>\n      <td>1</td>\n      <td>which player went to louisiana state university?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1)]</td>\n      <td>[Ben McDonald]</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nt-639</td>\n      <td>1</td>\n      <td>0</td>\n      <td>who are the players?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nt-639</td>\n      <td>1</td>\n      <td>1</td>\n      <td>which ones are in the top 26 picks?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nt-639</td>\n      <td>1</td>\n      <td>2</td>\n      <td>and of those, who is from louisiana state univ...</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1)]</td>\n      <td>[Ben McDonald]</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>nt-639</td>\n      <td>2</td>\n      <td>0</td>\n      <td>who are the players in the top 26?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>nt-639</td>\n      <td>2</td>\n      <td>1</td>\n      <td>of those, which one was from louisiana state u...</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1)]</td>\n      <td>[Ben McDonald]</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>nt-11649</td>\n      <td>0</td>\n      <td>0</td>\n      <td>what are all the names of the teams?</td>\n      <td>table_csv/204_135.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Cordoba CF, CD Malaga, Granada CF, UD Las Pal...</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>nt-11649</td>\n      <td>0</td>\n      <td>1</td>\n      <td>of these, which teams had any losses?</td>\n      <td>table_csv/204_135.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Cordoba CF, CD Malaga, Granada CF, UD Las Pal...</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>nt-11649</td>\n      <td>0</td>\n      <td>2</td>\n      <td>of these teams, which had more than 21 losses?</td>\n      <td>table_csv/204_135.csv</td>\n      <td>[(15, 1)]</td>\n      <td>[CD Villarrobledo]</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944637194},"id":"zFsYhGKRQ7l2","outputId":"311b271d-66bb-4a34-f67e-2c3c5ae175ab"}},{"cell_type":"markdown","source":["Let's create a new dataframe that groups questions which are asked in a sequence related to the table. We can do this by adding a sequence_id column, which is a combination of the id and annotator columns:"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"wjiHHznjQ7l4"}},{"cell_type":"code","source":["def get_sequence_id(example_id, annotator):\n","  if \"-\" in str(annotator):\n","    raise ValueError('\"-\" not allowed in annotator.')\n","  return f\"{example_id}-{annotator}\"\n","\n","train_data['sequence_id'] = train_data.apply(lambda x: get_sequence_id(x.id, x.annotator), axis=1)\n","train_data.head()"],"outputs":[{"output_type":"execute_result","execution_count":8,"data":{"text/plain":"       id  annotator  position  \\\n0  nt-639          0         0   \n1  nt-639          0         1   \n2  nt-639          1         0   \n3  nt-639          1         1   \n4  nt-639          1         2   \n\n                                            question             table_file  \\\n0                        where are the players from?  table_csv/203_149.csv   \n1   which player went to louisiana state university?  table_csv/203_149.csv   \n2                               who are the players?  table_csv/203_149.csv   \n3                which ones are in the top 26 picks?  table_csv/203_149.csv   \n4  and of those, who is from louisiana state univ...  table_csv/203_149.csv   \n\n                                  answer_coordinates  \\\n0  [(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...   \n1                                           [(0, 1)]   \n2  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n3  [(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...   \n4                                           [(0, 1)]   \n\n                                         answer_text sequence_id  \n0  [Louisiana State University, Valley HS (Las Ve...    nt-639-0  \n1                                     [Ben McDonald]    nt-639-0  \n2  [Ben McDonald, Tyler Houston, Roger Salkeld, J...    nt-639-1  \n3  [Ben McDonald, Tyler Houston, Roger Salkeld, J...    nt-639-1  \n4                                     [Ben McDonald]    nt-639-1  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>annotator</th>\n      <th>position</th>\n      <th>question</th>\n      <th>table_file</th>\n      <th>answer_coordinates</th>\n      <th>answer_text</th>\n      <th>sequence_id</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nt-639</td>\n      <td>0</td>\n      <td>0</td>\n      <td>where are the players from?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, 4...</td>\n      <td>[Louisiana State University, Valley HS (Las Ve...</td>\n      <td>nt-639-0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nt-639</td>\n      <td>0</td>\n      <td>1</td>\n      <td>which player went to louisiana state university?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1)]</td>\n      <td>[Ben McDonald]</td>\n      <td>nt-639-0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nt-639</td>\n      <td>1</td>\n      <td>0</td>\n      <td>who are the players?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n      <td>nt-639-1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>nt-639</td>\n      <td>1</td>\n      <td>1</td>\n      <td>which ones are in the top 26 picks?</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, 1...</td>\n      <td>[Ben McDonald, Tyler Houston, Roger Salkeld, J...</td>\n      <td>nt-639-1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>nt-639</td>\n      <td>1</td>\n      <td>2</td>\n      <td>and of those, who is from louisiana state univ...</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[(0, 1)]</td>\n      <td>[Ben McDonald]</td>\n      <td>nt-639-1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944643127},"id":"PyqO4a4uQ7l5","outputId":"cd3a0e6a-9f05-4cf2-d86a-af7f3a2edfe4"}},{"cell_type":"code","source":["# let's group table-question pairs by sequence id, and remove some columns we don't need \n","grouped = train_data.groupby(by='sequence_id').agg(lambda x: x.tolist())\n","grouped = grouped.drop(columns=['id', 'annotator', 'position'])\n","grouped['table_file'] = grouped['table_file'].apply(lambda x: x[0])\n","grouped.head(10)"],"outputs":[{"output_type":"execute_result","execution_count":9,"data":{"text/plain":"                                                      question  \\\nsequence_id                                                      \nns-1292-0    [who are all the athletes?, where are they fro...   \nns-1292-1    [who competed in the men's 50 kilometer walk a...   \nns-1292-2    [what competitors were from germany?, who was ...   \nns-3397-0    [what are the number of bronze medals received...   \nns-3397-1    [what are the countries of the 2004 african ju...   \nns-3397-2    [what are all the nations listed?, how many br...   \nns-3441-0    [where did calvin murray go to school?, where ...   \nns-3441-1    [what team was the last pick in the top 10, wh...   \nns-3441-2    [who are all of the players?, where did they a...   \nnt-10730-0   [what was the production numbers of each revol...   \n\n                        table_file  \\\nsequence_id                          \nns-1292-0    table_csv/204_521.csv   \nns-1292-1    table_csv/204_521.csv   \nns-1292-2    table_csv/204_521.csv   \nns-3397-0     table_csv/204_34.csv   \nns-3397-1     table_csv/204_34.csv   \nns-3397-2     table_csv/204_34.csv   \nns-3441-0    table_csv/203_149.csv   \nns-3441-1    table_csv/203_149.csv   \nns-3441-2    table_csv/203_149.csv   \nnt-10730-0   table_csv/203_253.csv   \n\n                                            answer_coordinates  \\\nsequence_id                                                      \nns-1292-0    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...   \nns-1292-1    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...   \nns-1292-2                         [[(3, 1), (5, 1)], [(3, 1)]]   \nns-3397-0    [[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ...   \nns-3397-1    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...   \nns-3397-2    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...   \nns-3441-0    [[(10, 4)], [(19, 4)], [(24, 4)], [(9, 4)], [(...   \nns-3441-1                                 [[(9, 2)], [(9, 1)]]   \nns-3441-2    [[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...   \nnt-10730-0   [[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ...   \n\n                                                   answer_text  \nsequence_id                                                     \nns-1292-0    [[Tommy Green, Janis Dalins, Ugo Frigerio, Kar...  \nns-1292-1    [[Tommy Green, Janis Dalins, Ugo Frigerio, Kar...  \nns-1292-2         [[Karl Hahnel, Paul Sievert], [Karl Hahnel]]  \nns-3397-0    [[0, 5, 3, 5, 5, 1, 0, 0, 0, 0, 3, 2, 1, 1, 1,...  \nns-3397-1    [[Algeria, Tunisia, Egypt, Cameroon, Morocco, ...  \nns-3397-2    [[Algeria, Tunisia, Egypt, Cameroon, Morocco, ...  \nns-3441-0    [[W.T. White High School (Dallas, TX)], [Unive...  \nns-3441-1                [[Montreal Expos], [Charles Johnson]]  \nns-3441-2    [[Ben McDonald, Tyler Houston, Roger Salkeld, ...  \nnt-10730-0   [[1,900 (estimated), 14,500 (estimated), 6,000...  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>question</th>\n      <th>table_file</th>\n      <th>answer_coordinates</th>\n      <th>answer_text</th>\n    </tr>\n    <tr>\n      <th>sequence_id</th>\n      <th></th>\n      <th></th>\n      <th></th>\n      <th></th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>ns-1292-0</th>\n      <td>[who are all the athletes?, where are they fro...</td>\n      <td>table_csv/204_521.csv</td>\n      <td>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...</td>\n      <td>[[Tommy Green, Janis Dalins, Ugo Frigerio, Kar...</td>\n    </tr>\n    <tr>\n      <th>ns-1292-1</th>\n      <td>[who competed in the men's 50 kilometer walk a...</td>\n      <td>table_csv/204_521.csv</td>\n      <td>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...</td>\n      <td>[[Tommy Green, Janis Dalins, Ugo Frigerio, Kar...</td>\n    </tr>\n    <tr>\n      <th>ns-1292-2</th>\n      <td>[what competitors were from germany?, who was ...</td>\n      <td>table_csv/204_521.csv</td>\n      <td>[[(3, 1), (5, 1)], [(3, 1)]]</td>\n      <td>[[Karl Hahnel, Paul Sievert], [Karl Hahnel]]</td>\n    </tr>\n    <tr>\n      <th>ns-3397-0</th>\n      <td>[what are the number of bronze medals received...</td>\n      <td>table_csv/204_34.csv</td>\n      <td>[[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ...</td>\n      <td>[[0, 5, 3, 5, 5, 1, 0, 0, 0, 0, 3, 2, 1, 1, 1,...</td>\n    </tr>\n    <tr>\n      <th>ns-3397-1</th>\n      <td>[what are the countries of the 2004 african ju...</td>\n      <td>table_csv/204_34.csv</td>\n      <td>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...</td>\n      <td>[[Algeria, Tunisia, Egypt, Cameroon, Morocco, ...</td>\n    </tr>\n    <tr>\n      <th>ns-3397-2</th>\n      <td>[what are all the nations listed?, how many br...</td>\n      <td>table_csv/204_34.csv</td>\n      <td>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...</td>\n      <td>[[Algeria, Tunisia, Egypt, Cameroon, Morocco, ...</td>\n    </tr>\n    <tr>\n      <th>ns-3441-0</th>\n      <td>[where did calvin murray go to school?, where ...</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[[(10, 4)], [(19, 4)], [(24, 4)], [(9, 4)], [(...</td>\n      <td>[[W.T. White High School (Dallas, TX)], [Unive...</td>\n    </tr>\n    <tr>\n      <th>ns-3441-1</th>\n      <td>[what team was the last pick in the top 10, wh...</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[[(9, 2)], [(9, 1)]]</td>\n      <td>[[Montreal Expos], [Charles Johnson]]</td>\n    </tr>\n    <tr>\n      <th>ns-3441-2</th>\n      <td>[who are all of the players?, where did they a...</td>\n      <td>table_csv/203_149.csv</td>\n      <td>[[(0, 1), (1, 1), (2, 1), (3, 1), (4, 1), (5, ...</td>\n      <td>[[Ben McDonald, Tyler Houston, Roger Salkeld, ...</td>\n    </tr>\n    <tr>\n      <th>nt-10730-0</th>\n      <td>[what was the production numbers of each revol...</td>\n      <td>table_csv/203_253.csv</td>\n      <td>[[(0, 4), (1, 4), (2, 4), (3, 4), (4, 4), (5, ...</td>\n      <td>[[1,900 (estimated), 14,500 (estimated), 6,000...</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944648337},"id":"xIVVdxoJQ7l6","outputId":"9cf332e8-619a-4873-e749-918201436cff"}},{"cell_type":"markdown","source":["Each row in the dataframe above now consists of a table and one or more questions which are asked in a sequence. Let's visualize the first row, i.e. a table, together with its queries:"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"ixMJwaTDQ7l6"}},{"cell_type":"code","source":["from IPython.display import display\n","\n","# path to the directory containing all csv files\n","table_csv_path = \"SQA Release 1.0/table_csv\"\n","\n","item = grouped.iloc[1]\n","table = pd.read_csv(table_csv_path + item.table_file[9:]).astype(str) \n","\n","display(table)\n","print(\"\")\n","print(item.question)"],"outputs":[{"output_type":"display_data","data":{"text/plain":"    Rank                Name    Nationality Time (hand) Notes\n0    nan         Tommy Green  Great Britain     4:50:10    OR\n1    nan        Janis Dalins         Latvia     4:57:20   nan\n2    nan        Ugo Frigerio          Italy     4:59:06   nan\n3    4.0         Karl Hahnel        Germany     5:06:06   nan\n4    5.0      Ettore Rivolta          Italy     5:07:39   nan\n5    6.0        Paul Sievert        Germany     5:16:41   nan\n6    7.0      Henri Quintric         France     5:27:25   nan\n7    8.0       Ernie Crosbie  United States     5:28:02   nan\n8    9.0       Bill Chisholm  United States     5:51:00   nan\n9   10.0       Alfred Maasik        Estonia     6:19:00   nan\n10   nan        Henry Cieman         Canada         nan   DNF\n11   nan        John Moralis         Greece         nan   DNF\n12   nan    Francesco Pretti          Italy         nan   DNF\n13   nan  Arthur Tell Schwab    Switzerland         nan   DNF\n14   nan        Harry Hinkel  United States         nan   DNF","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>Rank</th>\n      <th>Name</th>\n      <th>Nationality</th>\n      <th>Time (hand)</th>\n      <th>Notes</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>nan</td>\n      <td>Tommy Green</td>\n      <td>Great Britain</td>\n      <td>4:50:10</td>\n      <td>OR</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>nan</td>\n      <td>Janis Dalins</td>\n      <td>Latvia</td>\n      <td>4:57:20</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>nan</td>\n      <td>Ugo Frigerio</td>\n      <td>Italy</td>\n      <td>4:59:06</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4.0</td>\n      <td>Karl Hahnel</td>\n      <td>Germany</td>\n      <td>5:06:06</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5.0</td>\n      <td>Ettore Rivolta</td>\n      <td>Italy</td>\n      <td>5:07:39</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>6.0</td>\n      <td>Paul Sievert</td>\n      <td>Germany</td>\n      <td>5:16:41</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>7.0</td>\n      <td>Henri Quintric</td>\n      <td>France</td>\n      <td>5:27:25</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>8.0</td>\n      <td>Ernie Crosbie</td>\n      <td>United States</td>\n      <td>5:28:02</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>9.0</td>\n      <td>Bill Chisholm</td>\n      <td>United States</td>\n      <td>5:51:00</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>10.0</td>\n      <td>Alfred Maasik</td>\n      <td>Estonia</td>\n      <td>6:19:00</td>\n      <td>nan</td>\n    </tr>\n    <tr>\n      <th>10</th>\n      <td>nan</td>\n      <td>Henry Cieman</td>\n      <td>Canada</td>\n      <td>nan</td>\n      <td>DNF</td>\n    </tr>\n    <tr>\n      <th>11</th>\n      <td>nan</td>\n      <td>John Moralis</td>\n      <td>Greece</td>\n      <td>nan</td>\n      <td>DNF</td>\n    </tr>\n    <tr>\n      <th>12</th>\n      <td>nan</td>\n      <td>Francesco Pretti</td>\n      <td>Italy</td>\n      <td>nan</td>\n      <td>DNF</td>\n    </tr>\n    <tr>\n      <th>13</th>\n      <td>nan</td>\n      <td>Arthur Tell Schwab</td>\n      <td>Switzerland</td>\n      <td>nan</td>\n      <td>DNF</td>\n    </tr>\n    <tr>\n      <th>14</th>\n      <td>nan</td>\n      <td>Harry Hinkel</td>\n      <td>United States</td>\n      <td>nan</td>\n      <td>DNF</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}},{"output_type":"stream","name":"stdout","text":"\n[\"who competed in the men's 50 kilometer walk at the 1932 summer olympics?\", \"of those competitors in the 1932 summer olympics men's 50 kilometer walk, which ones were from germany?\", 'of those competitors from germany, which was not paul sievert?']\n"}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944667642},"id":"Gd_Fac-zQ7l7","outputId":"0c0d62d9-0a8f-4e45-98a8-58c447d46806"}},{"cell_type":"markdown","source":["We can see that there are 3 sequential questions asked related to the contents of the table.\n","\n","We can now use TapasTokenizer to batch encode this, as follows:"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"amBlpMYuQ7l7"}},{"cell_type":"code","source":["import torch\n","from transformers import TapasTokenizer\n","\n","# initialize the tokenizer\n","tokenizer = TapasTokenizer.from_pretrained(\"google/tapas-base\")"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944670985},"id":"7mZAYOSkQ7l8"}},{"cell_type":"code","source":["encoding = tokenizer(table=table, queries=item.question, answer_coordinates=item.answer_coordinates, answer_text=item.answer_text,\n","                     truncation=True, padding=\"max_length\", return_tensors=\"pt\")\n","encoding.keys()"],"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"dict_keys(['input_ids', 'labels', 'numeric_values', 'numeric_values_scale', 'token_type_ids', 'attention_mask'])"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944673414},"id":"nJnGZLeqQ7l8","outputId":"ec1b33f0-c868-4dfa-a5d0-564f8a57ad00"}},{"cell_type":"markdown","source":["TAPAS basically flattens every table-question pair before feeding it into a BERT like model:"],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"A_IYlfQVQ7l9"}},{"cell_type":"code","source":["tokenizer.decode(encoding[\"input_ids\"][0])"],"outputs":[{"output_type":"execute_result","execution_count":14,"data":{"text/plain":"\"[CLS] who competed in the men's 50 kilometer walk at the 1932 summer olympics? [SEP] rank name nationality time ( hand ) notes [EMPTY] tommy green great britain 4 : 50 : 10 or [EMPTY] janis dalins latvia 4 : 57 : 20 [EMPTY] [EMPTY] ugo frigerio italy 4 : 59 : 06 [EMPTY] 4. 0 karl hahnel germany 5 : 06 : 06 [EMPTY] 5. 0 ettore rivolta italy 5 : 07 : 39 [EMPTY] 6. 0 paul sievert germany 5 : 16 : 41 [EMPTY] 7. 0 henri quintric france 5 : 27 : 25 [EMPTY] 8. 0 ernie crosbie united states 5 : 28 : 02 [EMPTY] 9. 0 bill chisholm united states 5 : 51 : 00 [EMPTY] 10. 0 alfred maasik estonia 6 : 19 : 00 [EMPTY] [EMPTY] henry cieman canada [EMPTY] dnf [EMPTY] john moralis greece [EMPTY] dnf [EMPTY] francesco pretti italy [EMPTY] dnf [EMPTY] arthur tell schwab switzerland [EMPTY] dnf [EMPTY] harry hinkel united states [EMPTY] dnf [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD] [PAD]\""},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944676590},"id":"-WkCLJSxQ7l-","outputId":"77ef98d6-29e7-4712-8b53-c99ab365b2b1"}},{"cell_type":"markdown","source":["Let's create a PyTorch dataset and corresponding dataloader. Note the getitem method here: in order to properly set the prev_labels token types, we must check whether a table-question pair is the first in a sequence or not. In case it is, we can just encode it. In case it isn't, we need to encode it together with the previous table-question pair."],"metadata":{"nteract":{"transient":{"deleting":false}},"id":"CEz4nWUKQ7mB"}},{"cell_type":"code","source":["class TableDataset(torch.utils.data.Dataset):\n","    def __init__(self, df, tokenizer):\n","        self.df = df\n","        self.tokenizer = tokenizer\n","\n","    def __getitem__(self, idx):\n","        item = self.df.iloc[idx]\n","        table = pd.read_csv(table_csv_path + item.table_file[9:]).astype(str) # TapasTokenizer expects the table data to be text only\n","        if item.position != 0:\n","          # use the previous table-question pair to correctly set the prev_labels token type ids\n","          previous_item = self.df.iloc[idx-1]\n","          encoding = self.tokenizer(table=table, \n","                                    queries=[previous_item.question, item.question], \n","                                    answer_coordinates=[previous_item.answer_coordinates, item.answer_coordinates], \n","                                    answer_text=[previous_item.answer_text, item.answer_text],\n","                                    padding=\"max_length\",\n","                                    truncation=True,\n","                                    return_tensors=\"pt\"\n","          )\n","          # use encodings of second table-question pair in the batch\n","          encoding = {key: val[-1] for key, val in encoding.items()}\n","        else:\n","          # this means it's the first table-question pair in a sequence\n","          encoding = self.tokenizer(table=table, \n","                                    queries=item.question, \n","                                    answer_coordinates=item.answer_coordinates, \n","                                    answer_text=item.answer_text,\n","                                    padding=\"max_length\",\n","                                    truncation=True,\n","                                    return_tensors=\"pt\"\n","          )\n","          # remove the batch dimension which the tokenizer adds \n","          encoding = {key: val.squeeze(0) for key, val in encoding.items()}\n","        return encoding\n","\n","    def __len__(self):\n","        return len(self.df)\n","\n","train_dataset = TableDataset(df=train_data, tokenizer=tokenizer)\n","train_dataloader = torch.utils.data.DataLoader(train_dataset, batch_size=2)"],"outputs":[],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944685288},"id":"hKX24YP5Q7mB"}},{"cell_type":"markdown","source":["### Define the Model"],"metadata":{"id":"1cQyJwpGS_pB"}},{"cell_type":"code","source":["from transformers import TapasConfig, TapasForQuestionAnswering\n","\n","# the base  model with default SQA configuration\n","model = TapasForQuestionAnswering.from_pretrained(\"google/tapas-base\")\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","model.to(device)"],"outputs":[{"output_type":"stream","name":"stderr","text":"Some weights of TapasForQuestionAnswering were not initialized from the model checkpoint at google/tapas-base and are newly initialized: ['output_weights', 'column_output_bias', 'column_output_weights', 'output_bias']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"},{"output_type":"execute_result","execution_count":2,"data":{"text/plain":"TapasForQuestionAnswering(\n  (tapas): TapasModel(\n    (embeddings): TapasEmbeddings(\n      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n      (position_embeddings): Embedding(1024, 768)\n      (token_type_embeddings_0): Embedding(3, 768)\n      (token_type_embeddings_1): Embedding(256, 768)\n      (token_type_embeddings_2): Embedding(256, 768)\n      (token_type_embeddings_3): Embedding(2, 768)\n      (token_type_embeddings_4): Embedding(256, 768)\n      (token_type_embeddings_5): Embedding(256, 768)\n      (token_type_embeddings_6): Embedding(10, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n      (dropout): Dropout(p=0.07, inplace=False)\n    )\n    (encoder): TapasEncoder(\n      (layer): ModuleList(\n        (0): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (1): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (2): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (3): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (4): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (5): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (6): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (7): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (8): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (9): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (10): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n        (11): TapasLayer(\n          (attention): TapasAttention(\n            (self): TapasSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.0, inplace=False)\n            )\n            (output): TapasSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n              (dropout): Dropout(p=0.07, inplace=False)\n            )\n          )\n          (intermediate): TapasIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n          )\n          (output): TapasOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n            (dropout): Dropout(p=0.07, inplace=False)\n          )\n        )\n      )\n    )\n    (pooler): TapasPooler(\n      (dense): Linear(in_features=768, out_features=768, bias=True)\n      (activation): Tanh()\n    )\n  )\n  (dropout): Dropout(p=0.07, inplace=False)\n)"},"metadata":{}}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650944611266},"id":"2EYIijkYQ7lq","outputId":"5ed1eb0f-698f-4810-b8df-4d692eba3a14"}},{"cell_type":"markdown","source":["### Train the Model"],"metadata":{"id":"QNz6T7fuUjAF"}},{"cell_type":"code","source":["from transformers import AdamW\n","\n","optimizer = AdamW(model.parameters(), lr=5e-5)\n","\n","for epoch in range(10):  # loop over the dataset multiple times\n","   print(\"Epoch:\", epoch)\n","   for idx, batch in enumerate(train_dataloader):\n","        # get the inputs;\n","        input_ids = batch[\"input_ids\"].to(device)\n","        attention_mask = batch[\"attention_mask\"].to(device)\n","        token_type_ids = batch[\"token_type_ids\"].to(device)\n","        labels = batch[\"labels\"].to(device)\n","        \n","        # zero the parameter gradients\n","        optimizer.zero_grad()\n","        # forward + backward + optimize\n","        outputs = model(input_ids=input_ids, attention_mask=attention_mask, token_type_ids=token_type_ids,\n","                       labels=labels)\n","        loss = outputs.loss\n","        print(\"Loss:\", loss.item())\n","        loss.backward()\n","        optimizer.step()"],"outputs":[{"output_type":"stream","name":"stdout","text":"Epoch: 0\nLoss: 2.4576611518859863\nLoss: 2.208237648010254\nLoss: 1.532903790473938\nLoss: 1.6185946464538574\nLoss: 1.4657636880874634\nLoss: 2.6599626541137695\nLoss: 2.347184419631958\nLoss: 2.606064796447754\nLoss: 2.5185210704803467\nLoss: 3.0615274906158447\nLoss: 2.393543243408203\nLoss: 2.137967586517334\nLoss: 2.0683250427246094\nLoss: 1.0513044595718384\nLoss: 4.0133466720581055\nLoss: 0.9346457719802856\nLoss: 1.0688081979751587\nLoss: 1.3251779079437256\nLoss: 2.0984935760498047\nLoss: 0.9930031299591064\nLoss: 0.8833472728729248\nLoss: 0.9931430816650391\nLoss: 0.9654159545898438\nLoss: 0.9942173957824707\nLoss: 4.04036808013916\nLoss: 0.7319822311401367\nLoss: 0.8009426593780518\nLoss: 2.230072259902954\nLoss: 0.6047083139419556\nLoss: 2.072066307067871\nLoss: 2.064149856567383\nLoss: 0.8191809058189392\nLoss: 0.8792343139648438\nLoss: 1.3855931758880615\nLoss: 1.428628921508789\nLoss: 1.130543828010559\nLoss: 1.8510346412658691\nLoss: 2.871675968170166\nLoss: 0.6975874900817871\nLoss: 0.5966156721115112\nLoss: 1.1064704656600952\nLoss: 1.4437670707702637\nLoss: 2.6413655281066895\nLoss: 1.6964423656463623\nLoss: 0.7680790424346924\nLoss: 1.0180079936981201\nLoss: 0.9175381660461426\nLoss: 3.1376564502716064\nLoss: 0.744032621383667\nLoss: 1.3005536794662476\nLoss: 0.6061826944351196\nLoss: 0.600135326385498\nLoss: 0.7182793617248535\nLoss: 0.4231736958026886\nLoss: 1.3509199619293213\nLoss: 0.9552471041679382\nLoss: 1.4783720970153809\nLoss: 1.771835207939148\nLoss: 1.7006559371948242\nLoss: 1.2100012302398682\nLoss: 1.059225082397461\nLoss: 2.0328168869018555\nLoss: 2.4351112842559814\nLoss: 0.5943935513496399\nLoss: 0.821649968624115\nLoss: 0.2728615701198578\nLoss: 0.44912880659103394\nLoss: 0.5462073087692261\nLoss: 0.3408750295639038\nLoss: 0.6879234313964844\nLoss: 0.8903309106826782\nLoss: 0.4558773636817932\nLoss: 1.6096830368041992\nLoss: 0.5163788199424744\nLoss: 2.760603904724121\nLoss: 0.33665013313293457\nLoss: 0.4998423159122467\nLoss: 1.3968579769134521\nLoss: 0.6340229511260986\nLoss: 0.42674508690834045\nLoss: 0.5777251124382019\nLoss: 1.7342089414596558\nLoss: 0.437417596578598\nLoss: 1.4279264211654663\nLoss: 0.2491108775138855\nLoss: 1.1295688152313232\nLoss: 4.971898078918457\nLoss: 3.2023324966430664\nLoss: 3.3217973709106445\nEpoch: 1\nLoss: 2.3381009101867676\nLoss: 2.7957348823547363\nLoss: 2.2660794258117676\nLoss: 2.5829241275787354\nLoss: 2.854264736175537\nLoss: 2.9729466438293457\nLoss: 2.6388678550720215\nLoss: 3.3830230236053467\nLoss: 2.574519157409668\nLoss: 2.6861748695373535\nLoss: 2.519805908203125\nLoss: 2.277039051055908\nLoss: 2.2868309020996094\nLoss: 2.325270414352417\nLoss: 2.467172622680664\nLoss: 1.5747179985046387\nLoss: 1.7166335582733154\nLoss: 1.2057791948318481\nLoss: 2.4366025924682617\nLoss: 1.702760100364685\nLoss: 1.277221441268921\nLoss: 0.7861801981925964\nLoss: 0.7861282825469971\nLoss: 0.7069176435470581\nLoss: 4.422350883483887\nLoss: 1.8971168994903564\nLoss: 1.7801302671432495\nLoss: 1.7634258270263672\nLoss: 0.6335442066192627\nLoss: 2.3459677696228027\nLoss: 3.343989372253418\nLoss: 0.9676385521888733\nLoss: 0.7847630381584167\nLoss: 2.2324295043945312\nLoss: 1.1433788537979126\nLoss: 1.1502737998962402\nLoss: 1.4102940559387207\nLoss: 1.7845219373703003\nLoss: 1.172404170036316\nLoss: 0.4797315001487732\nLoss: 1.0034675598144531\nLoss: 1.1820170879364014\nLoss: 1.0728893280029297\nLoss: 0.8394725918769836\nLoss: 0.45156484842300415\nLoss: 0.6858947277069092\nLoss: 0.6362557411193848\nLoss: 0.8810835480690002\nLoss: 0.986376166343689\nLoss: 1.906964659690857\nLoss: 0.26378312706947327\nLoss: 0.48328134417533875\nLoss: 1.1336673498153687\nLoss: 0.36913275718688965\nLoss: 2.0964717864990234\nLoss: 1.9209949970245361\nLoss: 1.092889428138733\nLoss: 1.092160701751709\nLoss: 3.1710925102233887\nLoss: 1.3558262586593628\nLoss: 1.1108787059783936\nLoss: 1.3908734321594238\nLoss: 3.1798901557922363\nLoss: 2.5206966400146484\nLoss: 2.228146553039551\nLoss: 1.515864372253418\nLoss: 1.0462549924850464\nLoss: 0.5872219204902649\nLoss: 0.8317954540252686\nLoss: 1.055379033088684\nLoss: 0.5670561790466309\nLoss: 0.7454215884208679\nLoss: 0.9469505548477173\nLoss: 0.7005804181098938\nLoss: 0.6282119750976562\nLoss: 0.6620094776153564\nLoss: 0.6396251916885376\nLoss: 0.87898850440979\nLoss: 0.8709713220596313\nLoss: 1.4577138423919678\nLoss: 2.1019811630249023\nLoss: 1.161744475364685\nLoss: 0.3340829610824585\nLoss: 0.8118714690208435\nLoss: 0.25834041833877563\nLoss: 2.7668399810791016\nLoss: 4.842058181762695\nLoss: 0.7876828908920288\nLoss: 5.296260833740234\nEpoch: 2\nLoss: 3.4253125190734863\nLoss: 2.5045454502105713\nLoss: 1.8428471088409424\nLoss: 1.5801399946212769\nLoss: 1.6029908657073975\nLoss: 1.515505313873291\nLoss: 1.8657151460647583\nLoss: 2.5151944160461426\nLoss: 1.9952352046966553\nLoss: 1.471211552619934\nLoss: 1.5568865537643433\nLoss: 1.407825231552124\nLoss: 1.0786129236221313\nLoss: 1.1929454803466797\nLoss: 3.9774813652038574\nLoss: 0.7760567665100098\nLoss: 0.798343300819397\nLoss: 0.8807529211044312\nLoss: 1.4361915588378906\nLoss: 0.875626802444458\nLoss: 0.8504940867424011\nLoss: 0.8609527349472046\nLoss: 0.7217743992805481\nLoss: 0.682960033416748\nLoss: 1.476135015487671\nLoss: 0.7233214378356934\nLoss: 0.6894859671592712\nLoss: 1.1501364707946777\nLoss: 0.7413545846939087\nLoss: 3.2116146087646484\nLoss: 3.0721077919006348\nLoss: 0.6533291339874268\nLoss: 0.5578804016113281\nLoss: 1.0527387857437134\nLoss: 0.6536382436752319\nLoss: 0.6677359342575073\nLoss: 0.9704769849777222\nLoss: 1.0160328149795532\nLoss: 0.5418831706047058\nLoss: 0.5108104348182678\nLoss: 0.5165084004402161\nLoss: 1.090627908706665\nLoss: 1.3863098621368408\nLoss: 1.4979749917984009\nLoss: 1.101876974105835\nLoss: 1.0651936531066895\nLoss: 1.2695871591567993\nLoss: 0.6401633024215698\nLoss: 0.524016261100769\nLoss: 0.7312189340591431\nLoss: 0.6890069246292114\nLoss: 0.6674965620040894\nLoss: 0.4527226686477661\nLoss: 0.39086151123046875\nLoss: 1.0117602348327637\nLoss: 0.9963843822479248\nLoss: 1.048182725906372\nLoss: 0.3715653419494629\nLoss: 2.0423200130462646\nLoss: 0.6200023889541626\nLoss: 3.401395320892334\nLoss: 4.513802528381348\nLoss: 0.8885016441345215\nLoss: 1.0176838636398315\nLoss: 1.1596208810806274\nLoss: 0.36658400297164917\nLoss: 0.6307181119918823\nLoss: 0.6234228014945984\nLoss: 0.552832841873169\nLoss: 0.6568209528923035\nLoss: 0.6077171564102173\nLoss: 0.38999098539352417\nLoss: 0.7644571661949158\nLoss: 0.31838589906692505\nLoss: 0.7177607417106628\nLoss: 0.2656627893447876\nLoss: 0.2427179217338562\nLoss: 0.5797964334487915\nLoss: 0.40942227840423584\nLoss: 0.3552207946777344\nLoss: 0.27262917160987854\nLoss: 2.6002719402313232\nLoss: 0.38400477170944214\nLoss: 1.8043314218521118\nLoss: 1.1451685428619385\nLoss: 1.1428542137145996\nLoss: 3.100083351135254\nLoss: 0.29205164313316345\nLoss: 1.7465128898620605\nEpoch: 3\nLoss: 2.882357120513916\nLoss: 2.533834934234619\nLoss: 1.3372009992599487\nLoss: 1.4508583545684814\nLoss: 1.0276292562484741\nLoss: 0.999143660068512\nLoss: 1.3857743740081787\nLoss: 3.364736795425415\nLoss: 2.627960205078125\nLoss: 2.429251194000244\nLoss: 2.117652654647827\nLoss: 2.369584560394287\nLoss: 0.8340975046157837\nLoss: 0.9371142983436584\nLoss: 1.696318507194519\nLoss: 0.7969540357589722\nLoss: 0.8265987634658813\nLoss: 0.8214970827102661\nLoss: 1.3244876861572266\nLoss: 0.7995324730873108\nLoss: 0.7305718660354614\nLoss: 0.7925999164581299\nLoss: 1.1534136533737183\nLoss: 1.0098456144332886\nLoss: 0.9174073934555054\nLoss: 0.6473084688186646\nLoss: 0.6423956155776978\nLoss: 0.6121540665626526\nLoss: 1.7124371528625488\nLoss: 0.8788478374481201\nLoss: 0.8958538770675659\nLoss: 0.6737712621688843\nLoss: 0.5374022126197815\nLoss: 0.596002995967865\nLoss: 0.7885833382606506\nLoss: 0.911605715751648\nLoss: 1.786909580230713\nLoss: 1.0736243724822998\nLoss: 0.4780484437942505\nLoss: 0.48093605041503906\nLoss: 0.6996641159057617\nLoss: 0.7081775665283203\nLoss: 2.14278507232666\nLoss: 1.8222191333770752\nLoss: 0.5586080551147461\nLoss: 0.6636168360710144\nLoss: 1.6447826623916626\nLoss: 2.9224674701690674\nLoss: 0.582638144493103\nLoss: 2.316124200820923\nLoss: 1.063492774963379\nLoss: 0.5636194348335266\nLoss: 0.8449831008911133\nLoss: 0.41669943928718567\nLoss: 1.1434351205825806\nLoss: 0.6610540151596069\nLoss: 0.35185718536376953\nLoss: 1.2611730098724365\nLoss: 1.4559876918792725\nLoss: 1.0584925413131714\nLoss: 1.4947196245193481\nLoss: 2.8237521648406982\nLoss: 1.766037940979004\nLoss: 0.4347919821739197\nLoss: 0.9221886992454529\nLoss: 0.31052732467651367\nLoss: 0.8337034583091736\nLoss: 0.5127775073051453\nLoss: 0.764114260673523\nLoss: 0.6092458963394165\nLoss: 0.5963324904441833\nLoss: 0.6503387689590454\nLoss: 1.1071641445159912\nLoss: 0.6340875625610352\nLoss: 1.2106733322143555\nLoss: 0.5785931944847107\nLoss: 0.5744316577911377\nLoss: 1.194852352142334\nLoss: 0.5862108469009399\nLoss: 1.1931450366973877\nLoss: 0.7911869287490845\nLoss: 0.9201393127441406\nLoss: 0.4325396418571472\nLoss: 1.0333642959594727\nLoss: 0.9927656650543213\nLoss: 0.8054847717285156\nLoss: 3.0284292697906494\nLoss: 0.28851646184921265\nLoss: 3.0279271602630615\nEpoch: 4\nLoss: 3.7385129928588867\nLoss: 1.6886532306671143\nLoss: 0.9621862173080444\nLoss: 0.4878210425376892\nLoss: 0.6812254190444946\nLoss: 3.5571022033691406\nLoss: 2.9696295261383057\nLoss: 2.7825162410736084\nLoss: 2.9879860877990723\nLoss: 0.7782735228538513\nLoss: 1.1421899795532227\nLoss: 1.206713080406189\nLoss: 0.9991406798362732\nLoss: 0.9185788631439209\nLoss: 2.4131669998168945\nLoss: 0.8914704918861389\nLoss: 0.8929948806762695\nLoss: 0.7329579591751099\nLoss: 1.0550587177276611\nLoss: 1.0328718423843384\nLoss: 0.6447150707244873\nLoss: 0.7029043436050415\nLoss: 0.9995564222335815\nLoss: 0.7761912941932678\nLoss: 1.1218088865280151\nLoss: 0.668013334274292\nLoss: 0.6488552093505859\nLoss: 1.7779648303985596\nLoss: 0.8766167759895325\nLoss: 0.8146389722824097\nLoss: 0.963286817073822\nLoss: 0.5863762497901917\nLoss: 0.5476102828979492\nLoss: 0.6329199075698853\nLoss: 0.6424916982650757\nLoss: 0.6668953895568848\nLoss: 0.6040855646133423\nLoss: 0.8735281229019165\nLoss: 0.5574411153793335\nLoss: 0.5142576694488525\nLoss: 0.6368935108184814\nLoss: 0.48035645484924316\nLoss: 2.9947121143341064\nLoss: 2.137523889541626\nLoss: 0.7084226608276367\nLoss: 0.7611097693443298\nLoss: 1.9627513885498047\nLoss: 1.965396761894226\nLoss: 0.4917144477367401\nLoss: 1.2930270433425903\nLoss: 0.5056394934654236\nLoss: 1.057042121887207\nLoss: 1.2341090440750122\nLoss: 0.8910830616950989\nLoss: 0.5540062785148621\nLoss: 0.5141477584838867\nLoss: 0.5048704743385315\nLoss: 1.043985366821289\nLoss: 0.7958423495292664\nLoss: 0.6075152158737183\nLoss: 1.0307049751281738\nLoss: 2.7889292240142822\nLoss: 0.5429595112800598\nLoss: 0.4334537386894226\nLoss: 1.0747215747833252\nLoss: 0.26496678590774536\nLoss: 0.9466167092323303\nLoss: 0.5311293601989746\nLoss: 0.5652687549591064\nLoss: 0.5939146876335144\nLoss: 0.5519636869430542\nLoss: 0.518543004989624\nLoss: 0.5339241623878479\nLoss: 0.5749988555908203\nLoss: 0.7200459837913513\nLoss: 0.37174880504608154\nLoss: 0.34202784299850464\nLoss: 0.5616163015365601\nLoss: 0.422624409198761\nLoss: 0.3129956126213074\nLoss: 0.3229331970214844\nLoss: 0.6878741979598999\nLoss: 0.4130479097366333\nLoss: 0.6728057861328125\nLoss: 0.5799050331115723\nLoss: 0.3196961283683777\nLoss: 1.4598822593688965\nLoss: 0.23136912286281586\nLoss: 1.4425830841064453\nEpoch: 5\nLoss: 2.781865358352661\nLoss: 2.2553439140319824\nLoss: 1.331878900527954\nLoss: 0.2634885311126709\nLoss: 1.2494664192199707\nLoss: 1.493060827255249\nLoss: 2.837653636932373\nLoss: 2.4607093334198\nLoss: 1.9363783597946167\nLoss: 1.3939027786254883\nLoss: 1.2433091402053833\nLoss: 0.7186001539230347\nLoss: 0.8560729026794434\nLoss: 0.9171679019927979\nLoss: 1.593857765197754\nLoss: 0.8215138912200928\nLoss: 0.7698358297348022\nLoss: 0.598913848400116\nLoss: 0.5961722135543823\nLoss: 0.7925207614898682\nLoss: 0.4385080635547638\nLoss: 0.5991587042808533\nLoss: 0.8561820983886719\nLoss: 0.6923867464065552\nLoss: 0.5725911855697632\nLoss: 0.5437729358673096\nLoss: 0.3729788661003113\nLoss: 0.4380614757537842\nLoss: 0.797688901424408\nLoss: 0.37412989139556885\nLoss: 0.3242800533771515\nLoss: 0.4544832706451416\nLoss: 0.3859483003616333\nLoss: 0.2552572190761566\nLoss: 0.33609825372695923\nLoss: 1.6095163822174072\nLoss: 0.9172698259353638\nLoss: 1.6138027906417847\nLoss: 0.30437228083610535\nLoss: 0.3065207600593567\nLoss: 0.4399116039276123\nLoss: 0.36549168825149536\nLoss: 1.697524070739746\nLoss: 1.9387376308441162\nLoss: 0.3042917251586914\nLoss: 0.3310951590538025\nLoss: 0.3511590361595154\nLoss: 4.450740814208984\nLoss: 0.8289197087287903\nLoss: 3.883047103881836\nLoss: 0.23667506873607635\nLoss: 0.30048802495002747\nLoss: 0.31624937057495117\nLoss: 0.35389626026153564\nLoss: 0.22617091238498688\nLoss: 0.22164106369018555\nLoss: 0.6617079377174377\nLoss: 0.33109939098358154\nLoss: 0.9254468083381653\nLoss: 0.3637719750404358\nLoss: 0.9180629253387451\nLoss: 1.9583994150161743\nLoss: 0.4781179428100586\nLoss: 0.4148002862930298\nLoss: 0.2651062309741974\nLoss: 0.22177651524543762\nLoss: 0.35344594717025757\nLoss: 0.6113595962524414\nLoss: 0.2910690903663635\nLoss: 0.551640510559082\nLoss: 0.6372400522232056\nLoss: 0.18997545540332794\nLoss: 0.5721501708030701\nLoss: 0.27972736954689026\nLoss: 0.5429384112358093\nLoss: 0.19636502861976624\nLoss: 0.1952616423368454\nLoss: 0.683739423751831\nLoss: 0.3810375928878784\nLoss: 0.2833138108253479\nLoss: 0.21228966116905212\nLoss: 0.5083914995193481\nLoss: 1.1046648025512695\nLoss: 0.30412113666534424\nLoss: 0.2515013813972473\nLoss: 0.16776446998119354\nLoss: 1.4100428819656372\nLoss: 0.22183918952941895\nLoss: 0.23274728655815125\nEpoch: 6\nLoss: 3.8835837841033936\nLoss: 2.217219829559326\nLoss: 1.1673442125320435\nLoss: 0.6698890924453735\nLoss: 0.9244976043701172\nLoss: 0.8043490648269653\nLoss: 1.0641415119171143\nLoss: 1.61201810836792\nLoss: 2.0219218730926514\nLoss: 0.5599186420440674\nLoss: 0.6545281410217285\nLoss: 0.5944388508796692\nLoss: 0.5296873450279236\nLoss: 0.5182884931564331\nLoss: 1.5560256242752075\nLoss: 0.5202476978302002\nLoss: 0.6127191185951233\nLoss: 0.6862621307373047\nLoss: 0.3486146330833435\nLoss: 0.5615798234939575\nLoss: 0.5273646116256714\nLoss: 0.6753882169723511\nLoss: 0.4717351496219635\nLoss: 0.3720015287399292\nLoss: 0.24236959218978882\nLoss: 0.39609330892562866\nLoss: 0.32960739731788635\nLoss: 0.38390111923217773\nLoss: 1.2368595600128174\nLoss: 1.0579429864883423\nLoss: 0.25877684354782104\nLoss: 0.19064898788928986\nLoss: 0.20692315697669983\nLoss: 0.2271837294101715\nLoss: 0.43607640266418457\nLoss: 1.5894224643707275\nLoss: 0.22591909766197205\nLoss: 0.5720354318618774\nLoss: 0.2379341423511505\nLoss: 0.24097861349582672\nLoss: 0.4660303592681885\nLoss: 0.14372564852237701\nLoss: 0.3124218285083771\nLoss: 1.155988097190857\nLoss: 0.24694643914699554\nLoss: 0.1419578194618225\nLoss: 0.23017370700836182\nLoss: 2.2786314487457275\nLoss: 0.20159435272216797\nLoss: 1.664754867553711\nLoss: 0.2412692904472351\nLoss: 0.23068973422050476\nLoss: 0.14677873253822327\nLoss: 0.39739200472831726\nLoss: 0.17839695513248444\nLoss: 0.17680856585502625\nLoss: 0.1783694624900818\nLoss: 0.1966676563024521\nLoss: 0.986840546131134\nLoss: 0.30257922410964966\nLoss: 1.824931263923645\nLoss: 1.7680305242538452\nLoss: 0.5040040016174316\nLoss: 0.3814140558242798\nLoss: 0.19600698351860046\nLoss: 0.26625508069992065\nLoss: 0.1740342080593109\nLoss: 0.4824746251106262\nLoss: 0.21613192558288574\nLoss: 0.42972850799560547\nLoss: 0.641531765460968\nLoss: 0.13747382164001465\nLoss: 0.6042211651802063\nLoss: 0.16970859467983246\nLoss: 0.42240405082702637\nLoss: 0.15344469249248505\nLoss: 0.15992160141468048\nLoss: 0.5637613534927368\nLoss: 0.3248305916786194\nLoss: 0.3681683838367462\nLoss: 0.41501912474632263\nLoss: 0.6693176031112671\nLoss: 0.43527668714523315\nLoss: 0.28967952728271484\nLoss: 0.16235828399658203\nLoss: 0.1591382622718811\nLoss: 0.5401118397712708\nLoss: 0.1876802146434784\nLoss: 0.18210875988006592\nEpoch: 7\nLoss: 1.6833356618881226\nLoss: 0.7238370776176453\nLoss: 0.30113837122917175\nLoss: 0.14751000702381134\nLoss: 0.6268500685691833\nLoss: 0.6147054433822632\nLoss: 0.19341632723808289\nLoss: 0.8868843913078308\nLoss: 0.26747673749923706\nLoss: 0.4843379259109497\nLoss: 0.28781598806381226\nLoss: 0.18197666108608246\nLoss: 0.2866300642490387\nLoss: 0.23774446547031403\nLoss: 0.9003377556800842\nLoss: 0.13250073790550232\nLoss: 1.021986484527588\nLoss: 0.4365537762641907\nLoss: 0.1455962061882019\nLoss: 0.234519362449646\nLoss: 0.3088763952255249\nLoss: 0.40053316950798035\nLoss: 0.5146718621253967\nLoss: 0.12916381657123566\nLoss: 0.08559554815292358\nLoss: 0.4154151380062103\nLoss: 0.31644606590270996\nLoss: 0.102691650390625\nLoss: 0.416756808757782\nLoss: 0.4586280882358551\nLoss: 0.12032123655080795\nLoss: 0.15346041321754456\nLoss: 0.13756585121154785\nLoss: 0.1888558715581894\nLoss: 0.2294139266014099\nLoss: 0.21661995351314545\nLoss: 0.11282572150230408\nLoss: 0.28574812412261963\nLoss: 0.2333839237689972\nLoss: 0.2454201579093933\nLoss: 0.1230914518237114\nLoss: 0.12261374294757843\nLoss: 0.11592131108045578\nLoss: 0.6991599798202515\nLoss: 0.23258960247039795\nLoss: 0.14271280169487\nLoss: 0.22849160432815552\nLoss: 1.1992377042770386\nLoss: 0.17047496140003204\nLoss: 0.642427921295166\nLoss: 0.23819619417190552\nLoss: 0.21175381541252136\nLoss: 0.12154639512300491\nLoss: 0.39550381898880005\nLoss: 2.9018712043762207\nLoss: 0.20209857821464539\nLoss: 0.20239800214767456\nLoss: 0.5287395715713501\nLoss: 1.5356578826904297\nLoss: 0.49283990263938904\nLoss: 0.7359054684638977\nLoss: 1.194854497909546\nLoss: 1.1091744899749756\nLoss: 0.40894460678100586\nLoss: 0.8352830410003662\nLoss: 0.2454243302345276\nLoss: 0.3638630211353302\nLoss: 0.43736040592193604\nLoss: 0.4125208556652069\nLoss: 0.45824992656707764\nLoss: 0.5625898241996765\nLoss: 0.2736145257949829\nLoss: 0.7026697993278503\nLoss: 0.17190136015415192\nLoss: 0.3211539089679718\nLoss: 0.286425918340683\nLoss: 0.1432349979877472\nLoss: 0.5453833341598511\nLoss: 0.5040384531021118\nLoss: 0.18906143307685852\nLoss: 0.19245493412017822\nLoss: 0.9400315284729004\nLoss: 0.24175259470939636\nLoss: 0.2956288158893585\nLoss: 0.16648653149604797\nLoss: 0.2674573063850403\nLoss: 0.4312361180782318\nLoss: 0.1646551489830017\nLoss: 0.2012571543455124\nEpoch: 8\nLoss: 0.8381115198135376\nLoss: 0.5180293321609497\nLoss: 0.1848161518573761\nLoss: 0.0887983962893486\nLoss: 0.9662924408912659\nLoss: 0.1242353692650795\nLoss: 0.031482577323913574\nLoss: 0.8366603255271912\nLoss: 0.5875931978225708\nLoss: 0.5738168954849243\nLoss: 0.18557587265968323\nLoss: 0.32266300916671753\nLoss: 0.25163528323173523\nLoss: 0.1318051666021347\nLoss: 0.5173554420471191\nLoss: 0.13302727043628693\nLoss: 0.6040210127830505\nLoss: 0.5617438554763794\nLoss: 0.45695993304252625\nLoss: 0.8074414134025574\nLoss: 0.2930365204811096\nLoss: 0.29519227147102356\nLoss: 0.27268072962760925\nLoss: 0.134005606174469\nLoss: 0.08802740275859833\nLoss: 1.1811120510101318\nLoss: 0.3252629041671753\nLoss: 0.15782096982002258\nLoss: 0.36442309617996216\nLoss: 0.1683025062084198\nLoss: 0.15750157833099365\nLoss: 0.21606220304965973\nLoss: 0.16614452004432678\nLoss: 0.24148425459861755\nLoss: 0.43856412172317505\nLoss: 0.2138187438249588\nLoss: 0.12186159193515778\nLoss: 0.29094743728637695\nLoss: 0.234091117978096\nLoss: 0.23027580976486206\nLoss: 0.11988881230354309\nLoss: 0.13593436777591705\nLoss: 0.11290021985769272\nLoss: 0.2277332991361618\nLoss: 0.22243840992450714\nLoss: 0.12032990902662277\nLoss: 0.22074954211711884\nLoss: 0.5767920017242432\nLoss: 0.15848633646965027\nLoss: 0.470159649848938\nLoss: 0.22992879152297974\nLoss: 0.19644290208816528\nLoss: 0.11615455895662308\nLoss: 0.36105307936668396\nLoss: 3.509737014770508\nLoss: 1.5802501440048218\nLoss: 2.347642421722412\nLoss: 0.09184857457876205\nLoss: 0.2144135981798172\nLoss: 1.3573099374771118\nLoss: 0.9568021297454834\nLoss: 1.4699902534484863\nLoss: 0.4493846893310547\nLoss: 0.4614344835281372\nLoss: 0.18699152767658234\nLoss: 1.490157961845398\nLoss: 0.33647894859313965\nLoss: 0.7986916303634644\nLoss: 0.24262292683124542\nLoss: 0.7545768618583679\nLoss: 0.7824605703353882\nLoss: 0.12386849522590637\nLoss: 0.6326131820678711\nLoss: 0.2554289698600769\nLoss: 0.9155112504959106\nLoss: 0.2187451422214508\nLoss: 0.18441300094127655\nLoss: 1.089263916015625\nLoss: 0.3098788857460022\nLoss: 0.2187882959842682\nLoss: 0.2173219919204712\nLoss: 0.4282621145248413\nLoss: 0.25828513503074646\nLoss: 0.22302953898906708\nLoss: 0.21041736006736755\nLoss: 0.22508114576339722\nLoss: 0.599288821220398\nLoss: 0.34171801805496216\nLoss: 0.19095030426979065\nEpoch: 9\nLoss: 4.771688461303711\nLoss: 0.897550642490387\nLoss: 0.5300489664077759\nLoss: 0.11297614127397537\nLoss: 0.4627014398574829\nLoss: 0.5504011511802673\nLoss: 0.29792502522468567\nLoss: 0.38830500841140747\nLoss: 0.5335256457328796\nLoss: 0.42393529415130615\nLoss: 0.343701034784317\nLoss: 0.28510159254074097\nLoss: 0.2697299122810364\nLoss: 0.15155208110809326\nLoss: 0.8424669504165649\nLoss: 0.12879671156406403\nLoss: 0.5808677673339844\nLoss: 0.42921510338783264\nLoss: 0.08644978702068329\nLoss: 0.2000199854373932\nLoss: 0.33443552255630493\nLoss: 0.5304684042930603\nLoss: 0.3131391108036041\nLoss: 0.14049255847930908\nLoss: 0.10623195022344589\nLoss: 0.43785208463668823\nLoss: 0.3142104744911194\nLoss: 0.17488040030002594\nLoss: 0.6099833250045776\nLoss: 0.07494766265153885\nLoss: 0.2418922781944275\nLoss: 0.12544669210910797\nLoss: 0.16203846037387848\nLoss: 1.2564054727554321\nLoss: 0.3391690254211426\nLoss: 0.20938052237033844\nLoss: 0.04352586716413498\nLoss: 0.26852846145629883\nLoss: 0.23708342015743256\nLoss: 0.23683267831802368\nLoss: 0.12740792334079742\nLoss: 0.11046136915683746\nLoss: 0.295518696308136\nLoss: 0.5171928405761719\nLoss: 0.23225267231464386\nLoss: 0.11919466406106949\nLoss: 0.2281917929649353\nLoss: 0.5359274744987488\nLoss: 0.17765088379383087\nLoss: 0.36537399888038635\nLoss: 0.2432788610458374\nLoss: 0.18301931023597717\nLoss: 0.1154220849275589\nLoss: 0.6044340133666992\nLoss: 0.4948435127735138\nLoss: 0.27521222829818726\nLoss: 0.23464903235435486\nLoss: 0.10839695483446121\nLoss: 0.22076986730098724\nLoss: 0.36255884170532227\nLoss: 0.40018230676651\nLoss: 0.6026192903518677\nLoss: 0.3946989178657532\nLoss: 0.38830816745758057\nLoss: 1.2985239028930664\nLoss: 0.2241075336933136\nLoss: 0.14068064093589783\nLoss: 0.47766244411468506\nLoss: 0.21740077435970306\nLoss: 0.5254517793655396\nLoss: 0.5409579277038574\nLoss: 0.16124479472637177\nLoss: 0.4820421040058136\nLoss: 0.16167935729026794\nLoss: 0.9745263457298279\nLoss: 0.14061301946640015\nLoss: 0.1778646856546402\nLoss: 0.5687379240989685\nLoss: 0.30876725912094116\nLoss: 0.21340440213680267\nLoss: 0.2210642695426941\nLoss: 0.42211857438087463\nLoss: 0.7653639912605286\nLoss: 0.18954825401306152\nLoss: 0.17768535017967224\nLoss: 0.16949555277824402\nLoss: 1.004616141319275\nLoss: 0.14740459620952606\nLoss: 0.1696414202451706\n"}],"execution_count":null,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650946281209},"id":"BEOQxyJHQ7mF","outputId":"0c8733d8-51cc-4b19-fd6e-bd254146ccf4"}},{"cell_type":"markdown","source":["### Pushing to HuggingFace Hub"],"metadata":{"id":"CJiCd0lDUA8r"}},{"cell_type":"code","source":["!pip install huggingface_hub"],"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting huggingface_hub\n","  Downloading huggingface_hub-0.5.1-py3-none-any.whl (77 kB)\n","\u001b[?25l\r\u001b[K     |████▏                           | 10 kB 16.5 MB/s eta 0:00:01\r\u001b[K     |████████▍                       | 20 kB 18.2 MB/s eta 0:00:01\r\u001b[K     |████████████▋                   | 30 kB 11.2 MB/s eta 0:00:01\r\u001b[K     |████████████████▉               | 40 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████           | 51 kB 3.9 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████▎      | 61 kB 4.6 MB/s eta 0:00:01\r\u001b[K     |█████████████████████████████▌  | 71 kB 4.8 MB/s eta 0:00:01\r\u001b[K     |████████████████████████████████| 77 kB 2.8 MB/s \n","\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (2.23.0)\n","Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.11.3)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.64.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.6.0)\n","Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (21.3)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (4.2.0)\n","Requirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from huggingface_hub) (3.13)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.9->huggingface_hub) (3.0.8)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->huggingface_hub) (3.8.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2021.10.8)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (2.10)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (1.24.3)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->huggingface_hub) (3.0.4)\n","Installing collected packages: huggingface-hub\n","Successfully installed huggingface-hub-0.5.1\n"]}],"execution_count":1,"metadata":{"jupyter":{"source_hidden":false,"outputs_hidden":false},"nteract":{"transient":{"deleting":false}},"gather":{"logged":1650947175455},"colab":{"base_uri":"https://localhost:8080/"},"id":"WMQJ7QDeQ7mG","executionInfo":{"status":"ok","timestamp":1650967158816,"user_tz":-330,"elapsed":6426,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}},"outputId":"b6d0ff5e-473f-4eda-c03c-7cb5c3dd6bfc"}},{"cell_type":"code","source":["from huggingface_hub import notebook_login\n","\n","notebook_login()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":386,"referenced_widgets":["920399a4fb264c08af0ed2685463c6c1","c94c7048615d43cea37e7738faecf87c","0dc01e7fa93144cb8c6db72f78377dc6","1a57b1206cfd481aa6363b9d651dd3c2","44b37d956d344666970c6d2962714dd0","d7421ccc07ba44c29d1050bdfb21dc4c","f22b8c99a65c4b8abbcdf30499339ae9","51d892ae973f43ddaf6bc9cf13531b47","cbdff173a768483bb26d3bb847c2e6bb","ddf4f98083544521ae4cd5797a22bbc6","5c3cd9079b314b95ac71a7327ca29418","f2c0ea37f6ca42f6a9ed2179b6ad064b","4a295d6fcc0e4ed5abe839271413a757","e9afb7ddc2424689b385f05523cf183c","1784cbddf34245188c54379fa21cc92e","f9470b33acd1497ab05014e1cbf786dc","4f9783875d2645b8a284d1428f937eb6"]},"id":"IEb7O6UMinT2","executionInfo":{"status":"ok","timestamp":1650954354347,"user_tz":-330,"elapsed":1037,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}},"outputId":"363aec03-2ef6-484e-8b93-2fde629b47e7"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Login successful\n","Your token has been saved to /root/.huggingface/token\n","\u001b[1m\u001b[31mAuthenticated through git-credential store but this isn't the helper defined on your machine.\n","You might have to re-authenticate when pushing to the Hugging Face Hub. Run the following command in your terminal in case you want to set this credential helper as the default\n","\n","git config --global credential.helper store\u001b[0m\n"]}]},{"cell_type":"code","source":["model.push_to_hub('Meena/table-question-answering-tapas')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":226,"referenced_widgets":["d9da0a7567834d55ac4e5b4a65a708a8","a208c0a98dfb4de48381308f7c78f456","43f0569afaa0433ea2c201ded1768ec3","0140c38f51ab4dc9b5cedb84edd69516","9cac5f0f632c420b8546c8916f21c552","cf20414e3c2a44be8b6a43d203cfc450","f2be3088d3944ec499b5e19542cb6e6c","89124797d7a249578ed6aa32928f867b","0d74247407554e29878d675379039ac5","7dc805e4ae174aa69a657a6e043f27e5","3943665947f843d89c9a63b05aa71f89"]},"id":"wW03yOd-jHrF","executionInfo":{"status":"ok","timestamp":1650954813768,"user_tz":-330,"elapsed":410688,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}},"outputId":"698b6224-fcd9-4c31-8d8a-7ecac224f086"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/huggingface_hub/utils/_deprecation.py:43: FutureWarning: Pass token='table-question-answering-tapas' as keyword args. From version 0.7 passing these as positional arguments will result in an error\n","  FutureWarning,\n","/usr/local/lib/python3.7/dist-packages/huggingface_hub/hf_api.py:599: FutureWarning: `create_repo` now takes `token` as an optional positional argument. Be sure to adapt your code!\n","  FutureWarning,\n","Cloning https://huggingface.co/Meena/table-question-answering-tapas into local empty directory.\n"]},{"output_type":"display_data","data":{"text/plain":["Upload file pytorch_model.bin:   0%|          | 3.34k/422M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9da0a7567834d55ac4e5b4a65a708a8"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["To https://huggingface.co/Meena/table-question-answering-tapas\n","   c32d802..0f828a7  main -> main\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'https://huggingface.co/Meena/table-question-answering-tapas/commit/0f828a70eb3dd6daca8851fa171ad5c6b9a52d84'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":7}]},{"cell_type":"code","source":["tokenizer.push_to_hub('Meena/table-question-answering-tapas')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":87},"id":"dykJpii2m6rl","executionInfo":{"status":"ok","timestamp":1650955382157,"user_tz":-330,"elapsed":4797,"user":{"displayName":"Ajay Karthick","userId":"01254622535029219698"}},"outputId":"67a31eb7-6895-4ddf-ff45-693c2d29b7a3"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["To https://huggingface.co/Meena/table-question-answering-tapas\n","   0f828a7..11151eb  main -> main\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["'https://huggingface.co/Meena/table-question-answering-tapas/commit/11151eb21004a9d969d27381aea7f953a9c4bfd1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":9}]}],"metadata":{"kernelspec":{"display_name":"Python 3.8 - AzureML","language":"python","name":"python38-azureml"},"language_info":{"name":"python","version":"3.8.5","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"microsoft":{"host":{"AzureML":{"notebookHasBeenCompleted":true}}},"nteract":{"version":"nteract-front-end@1.0.0"},"colab":{"name":"2 Train Model.ipynb","provenance":[],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}